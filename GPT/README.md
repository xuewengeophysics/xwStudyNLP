# Generative Pre-trained Transformer

参考资料
==
论文1 [GPT-Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)<br>
论文2 [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)<br>

代码1 [openai/finetune-transformer-lm](https://github.com/openai/finetune-transformer-lm)<br>
代码2 [openai/gpt-2](https://github.com/openai/gpt-2)<br>
代码3 [ConnorJL/GPT2](https://github.com/ConnorJL/GPT2)<br>
