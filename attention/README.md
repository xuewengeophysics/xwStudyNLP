# Attention

参考资料
==

### 1、论文

1  [[Bahdanau et al. ICLR2015]Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473v7)<br>
2  [[Luong et al. EMNLP2015]Effective Approaches to Attention-based Neural Machine Translation](https://aclweb.org/anthology/D15-1166)<br>
3  [[Kelvin Xu et al. ICML2015]Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)<br>
4  [[Zichao Yang et al. 2016]Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)<br>
5  [[Vaswani et al. ICCL2017]Attention Is All You Need](https://arxiv.org/abs/1706.03762)<br>
6  [[Yequan Wang et al. 2016]Attention-based LSTM for Aspect-level Sentiment Classification](https://aclweb.org/anthology/D16-1058)<br>
7  [[Zhouhan Lin et al. 2017]A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)<br>

### 2、博客

1  [Nir Arbel/Attention in RNNs](https://medium.com/datadriveninvestor/attention-in-rnns-321fbcd64f05)  [对应翻译/AI公园/戎怀阳/使用详细的例子来理解RNN中的注意力机制](https://mp.weixin.qq.com/s/j21govyAwBQehmJYmSsYIw)<br>

### 3、代码

