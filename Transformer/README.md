# transformer

## 参考资料

论文1：[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)<br>
论文2：[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)<br>
源码1：[kimiyoung/transformer-xl](https://github.com/kimiyoung/transformer-xl)<br>

博客1：[Jay Alammar/The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer)<br>
博客2：[Miguel Romero Calvo/Dissecting BERT Part 1: The Encoder](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3)<br>
博客3：[harvardnlp/The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)<br>

博客4：[张俊林/放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较](https://zhuanlan.zhihu.com/p/54743941)

博客5：[机器翻译模型Transformer代码详细解析](https://blog.csdn.net/mijiaoxiaosan/article/details/74909076)
博客5对应源码：[Kyubyong/transformer](https://github.com/Kyubyong/transformer)

博客6：[文本分类实战之Transformer模型](https://www.cnblogs.com/jiangxinyang/p/10210813.html)
博客6对应源码：[jiangxinyang227/textClassifier/Transformer](https://github.com/jiangxinyang227/textClassifier/tree/master/Transformer)<br>

源码1：https://github.com/brightmart/text_classification/tree/master/a07_Transformer<br>
源码2：https://github.com/pengming617/text_classification/tree/master/model/transformer<br>
源码3：https://github.com/SamLynnEvans/Transformer
对应博客[Samuel Lynn-Evans/How to code The Transformer in Pytorch](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec)
中文翻译[大数据文摘/百闻不如一码！手把手教你用Python搭一个Transformer](https://mp.weixin.qq.com/s/zLptc5bvo3rY_Jvu-rA6Pg)<br>


